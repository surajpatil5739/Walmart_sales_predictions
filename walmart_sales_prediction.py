# -*- coding: utf-8 -*-
"""walmart-sales-prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17YK3IUeAduAAf0R2E5rLwMASD_mYmcyr
"""

# Jovian Commit Essentials
# Please retain and execute this cell without modifying the contents for `jovian.commit` to work
!pip install jovian --upgrade -q
import jovian
jovian.set_project('walmart-sales-prediction')
jovian.set_colab_id('17YK3IUeAduAAf0R2E5rLwMASD_mYmcyr')

"""# walmart-sales-prediction

Use the "Run" button to execute the code.

One challenge of modeling retail data is the need to make decisions based on limited history. If Christmas comes but once a year, so does the chance to see how strategic decisions impacted the bottom line.
In this recruiting competition, job-seekers are provided with historical sales data for 45 Walmart stores located in different regions. Each store contains many departments, and participants must project the sales for each department in each store. To add to the challenge, selected holiday markdown events are included in the dataset. These markdowns are known to affect sales, but it is challenging to predict which departments are affected and the extent of the impact.

We will follow following steps:


*   Download the dataset
*   Preparing a dataset for training

*   Training and interpreting the model
*   Hyperparameter Tuning and regularization

*   Making predictions on test data set
*   Making the submission file
"""

!pip install opendatasets --upgrade --quiet
!pip install scikit-learn xgboost graphviz lightgbm --upgrade --quiet

!pip install matplotlib numpy pandas plotly seaborn --quiet

# Commented out IPython magic to ensure Python compatibility.
import os
import pandas as pd
pd.set_option("display.max_columns", 120)
pd.set_option("display.max_rows", 120)
import opendatasets as od
import plotly.express as px
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

# % matplotlib inline

sns.set_style('darkgrid')
matplotlib.rcParams['font.size'] = 14
matplotlib.rcParams['figure.figsize'] = (10,6)
matplotlib.rcParams['figure.facecolor'] = '#00000000'

!pip install jovian --upgrade --quiet

import jovian

# Execute this to save new versions of the notebook
jovian.commit(project="walmart-sales-prediction")





"""# DOWNLOADING THE DATASET USING OPENDATASETS LIBRARY
We will download the dataset from the kaggle competition using opendatasets library. The link to kaggle competition is
https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting




"""

od.download('https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting')

"""Lets see which file are available for us after downloading"""

os.listdir('walmart-recruiting-store-sales-forecasting')

"""We can see that our data is available in zip file. So, lets unzip it."""

!unzip ./walmart-recruiting-store-sales-forecasting/train.csv.zip
!unzip ./walmart-recruiting-store-sales-forecasting/test.csv.zip
!unzip ./walmart-recruiting-store-sales-forecasting/features.csv.zip
!unzip ./walmart-recruiting-store-sales-forecasting/sampleSubmission.csv.zip

"""Making data frames of datasets available to us."""

train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')
sampleSubmission_df = pd.read_csv('sampleSubmission.csv')
features_df = pd.read_csv('features.csv')
stores_df = pd.read_csv('./walmart-recruiting-store-sales-forecasting/stores.csv')

train_df

stores_df

features_df

test_df

sampleSubmission_df

"""# Merging the data

We can see that our dataset is is available in diffferrent datasets and to operations on it we will have to merge it.
"""

# first we will merge store_df and train_df
# then we will merge features_df with it
train_input_df = train_df.merge(stores_df, how = 'left').merge(features_df, how = 'left')

train_input_df.describe()

train_input_df.info()

#Now we will merge test_df with store_df
#Then we will merge it with features_df
test_input_df = test_df.merge(stores_df , how = 'left').merge(features_df , how = 'left')

test_input_df

"""# Data Exploration

Now we will be exploring the data. So that we will get the idea of how the data is related to each other.
"""

plt.figure()
plt.scatter(train_input_df[ 'Unemployment'] , y = train_input_df['Weekly_Sales'])
plt.title('unempoyment vs. weekly sales')
plt.xlabel('unemployment')
plt.ylabel('weekly sales')

plt.figure()
plt.scatter(train_input_df[ 'Type'] , y = train_input_df['Weekly_Sales'])
plt.title('type of store vs. weekly sales')
plt.xlabel('type of store')
plt.ylabel('weekly sales')

plt.title('no of each type of store')
sns.countplot(train_input_df['Type'])

plt.title('no of rows per year')
sns.countplot(pd.to_datetime(train_input_df.Date).dt.year)

plt.figure()
plt.scatter(train_input_df[ 'CPI'] , y = train_input_df['Weekly_Sales'])
plt.title('CPI vs. weekly sales')
plt.xlabel('CPI')
plt.ylabel('weekly sales')

fig = px.histogram(train_input_df , x = 'CPI' , y = 'Weekly_Sales')
fig.show()

def scatter(dataset , columns):
  plt.figure()
  plt.scatter(dataset[columns] , y = dataset['Weekly_Sales'])
  plt.title('{} vs. weekly sales'.format(columns))
  plt.xlabel('{}'.format(columns))
  plt.ylabel('weekly sales')

scatter(train_input_df , 'MarkDown1')
scatter(train_input_df , 'MarkDown2')
scatter(train_input_df , 'MarkDown3')
scatter(train_input_df , 'MarkDown4')
scatter(train_input_df , 'MarkDown5')

scatter(train_input_df, 'Fuel_Price')

sns.lineplot( x = 'Size' , y = 'Weekly_Sales' , data= train_input_df)

scatter(train_input_df , 'Temperature')

scatter(train_input_df , 'IsHoliday')

train_input_df.corr()

#using heatmap for correlation
sns.heatmap(train_input_df.corr() , cmap= 'Reds')

"""# DATA MANIPULATION

In this part we will be searching and putting the missing data. And we will be adding some columns which will be derived from the existing data. we will also do data imputation
"""

train_input_df[['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5']] = train_input_df[['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5']].fillna(0)
test_input_df[['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5']] = test_input_df[['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5']].fillna(0)

train_input_df[['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5']].isna().sum()

"""If you will see the data then you will find some weekly sales values less than zero. But values of our sales can not be less than zero. So, we will have to make negative values zero."""

train_input_df['Weekly_Sales'] = train_input_df['Weekly_Sales'].apply(lambda x: 0 if x < 0 else x)

train_input_df['Weekly_Sales'].describe()

test_input_df[['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5']].isna().sum()



train_input_df[['CPI','Unemployment','Fuel_Price']].isna().sum()

test_input_df[['CPI','Unemployment','Fuel_Price']].isna().sum()

"""from you we will get to know that all values of CPI and Unemployment are available in training dataset whereas it some values are missing in test dataset. we will put the values by taking average of all values using simplerImputer. I will perform this in later stage

Now we will make new column of week from the date column which we have.
"""

train_input_df['week'] = pd.to_datetime(train_input_df['Date']).dt.week
test_input_df['week'] = pd.to_datetime(test_input_df['Date']).dt.week

train_input_df

test_input_df



"""Here I will insert the missing values in test dataset"""

num_cols = test_input_df.select_dtypes(include=np.number).columns.tolist()

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy = 'mean')
imputer.fit(test_input_df[num_cols])

test_input_df[num_cols] = imputer.transform(test_input_df[num_cols])



"""Now I will make new datasets by dropping the Date column which will not be required."""

new_dataset = train_input_df.drop(columns=['Date'])
new_test = test_input_df.drop(columns=['Date'])

new_dataset

new_test

new_dataset.info()

"""Now I will seperate out the inputs and the targets in our training dataset."""



dataset_input = new_dataset.drop(columns=['Weekly_Sales'])
dataset_target = new_dataset['Weekly_Sales']

dataset_target

dataset_input

"""Now lets seperate out the numerical columns and categorical columns"""

num_cols = dataset_input.select_dtypes(include=np.number).columns.tolist()
num_cols

cat_cols = dataset_input.select_dtypes(['bool','object']).columns.tolist()
cat_cols

"""Now we will bring the range of all the numerical columns to (0,1) by using minmaxscaler"""

from sklearn.preprocessing import MinMaxScaler
scalar = MinMaxScaler()
scalar.fit(dataset_input[num_cols])

dataset_input[num_cols] = scalar.transform(dataset_input[num_cols])

dataset_input.describe()

scalar.fit(new_test[num_cols])

new_test[num_cols] = scalar.transform(new_test[num_cols])

new_test

new_test.describe()

"""Now we will do the encoding of the categorical columns using the onehotencoding."""

from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder(sparse = False , handle_unknown = 'ignore')

encoder.fit(dataset_input[cat_cols])

encoded_cols = list(encoder.get_feature_names(cat_cols))
encoded_cols

dataset_input[encoded_cols] = encoder.transform(dataset_input[cat_cols])

dataset_input

encoder.fit(new_test[cat_cols])

new_test[encoded_cols] = encoder.transform(new_test[cat_cols])

new_test



"""# Training The Model

Now lets train our Model.
"""

import sklearn

from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(n_jobs= -1, random_state = 42)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# model.fit(dataset_input[num_cols + encoded_cols] , dataset_target)

preds = model.predict(dataset_input[num_cols + encoded_cols])

print(model.max_leaf_nodes)

print(model.min_impurity_decrease)



from sklearn.metrics import mean_squared_error
rmse = mean_squared_error(dataset_target, preds , squared= False)

rmse



imp_df  = pd.DataFrame({'features' : dataset_input[num_cols + encoded_cols].columns , 'importance' : model.feature_importances_}).sort_values('importance', ascending = False)

imp_df





"""Here I tried using k cross validation but it took much time and my RAM collapsed so i avoided further using it."""

#from sklearn.model_selection import KFold

'''%%time
def train_and_evaluate(x_train, train_targets , x_val , val_targets , **params):
  model = RandomForestRegressor(random_state=42 , n_jobs= -1 , **params)
  model.fit(x_train , train_targets)
  train_rmse = mean_squared_error(model.predict(x_train) , train_targets,squared=False)
  val_rmse = mean_squared_error(model.predict(x_val) , val_targets , squared= False)
  return model,train_rmse , val_rmse'''

#kfold = KFold(n_splits = 5)

'''%%time
models = []

for train_idxs , val_idxs in kfold.split(dataset_input[num_cols + encoded_cols]):
  x_train , train_targets = dataset_input[num_cols + encoded_cols].iloc[train_idxs] , dataset_target.iloc[train_idxs]
  x_val , val_targets = dataset_input[num_cols + encoded_cols].iloc[val_idxs] , dataset_target.iloc[val_idxs]
  model, train_rmse, val_rmse = train_and_evaluate(x_train, 
                                                     train_targets, 
                                                     x_val, 
                                                     val_targets,
                                                   n_estimators = 50)
  models.append(model)
  print('Train RMSE: {}, Validation RMSE: {}'.format(train_rmse, val_rmse))'''

'''def predict_avg(models , inputs):
  return np.mean([model.predict(inputs) for model in models] , axis= 0)'''

'''preds = predict_avg(models , dataset_input[num_cols + encoded_cols])'''

#preds

#rmse = mean_squared_error(dataset_target , preds , squared= False)

#rmse



"""Now we will split our training data in training and validation dataset to do the hyperparameter tuning."""

from sklearn.model_selection import train_test_split

X_train,  X_val , train_targets , val_targets = train_test_split(dataset_input[num_cols + encoded_cols] , dataset_target , test_size = 0.1)

"""Defining helper functions for parameter tuning."""

def test_params(**params):
  model = RandomForestRegressor(n_jobs= -1 , random_state=42 , **params)
  model.fit(X_train , train_targets)
  train_rmse = mean_squared_error(train_targets , model.predict(X_train) , squared=False)
  val_rmse = mean_squared_error(val_targets , model.predict(X_val) , squared= False)
  return train_rmse , val_rmse

def test_params_and_plot(param_name , param_values):
  train_errors, val_errors = [], [] 
  for value in param_values :
    params  = { param_name : value}
    train_rmse , val_rmse = test_params(**params)
    train_errors.append(train_rmse)
    val_errors.append(val_rmse)
  plt.figure(figsize=(10,6))
  plt.title('Overfitting curve : ' + param_name)
  plt.plot(param_values , train_errors , 'b-o')
  plt.plot(param_values , val_errors , 'r-o')
  plt.xlabel(param_name)
  plt.ylabel('rmse')
  plt.legend(['Training', 'validation'])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# test_params_and_plot('n_estimators' , [20 , 40 , 60 , 80,100,120])

test_params_and_plot('max_depth', [5, 10, 15, 20, 25, 30, 35])

test_params_and_plot('max_leaf_nodes' , [ 2^80 , 2^100,  2^150, 2^200 , 2^250 , 2^300 , 2^400 , 2^500 , 2^700])

test_params_and_plot('max_samples' , [0.6, 0.7 , 0.8 , 0.9 , 0.95])

test_params_and_plot('max_features' , [0.5, 0.6,0.7, 0.8 , 0.9 , 0.95 ])

?RandomForestRegressor

test_params_and_plot('min_samples_leaf' , [2 , 4 , 8 , 16 , 32 , 64 , 128])

test_params_and_plot('min_samples_split' , [3 , 5 , 10 , 20 , 40 , 80 , 160])

test_params_and_plot('min_impurity_decrease' , [1e-2 , 1e-4 , 1e-6 , 1e-8])



model = RandomForestRegressor(n_jobs= -1 , 
                              random_state=42 , 
                              n_estimators = 60 , 
                              max_depth = 20 ,
                              max_leaf_nodes = None , 
                              max_samples = 0.8 ,
                              max_features = 0.7 , 
                              min_samples_leaf =  1 ,
                              min_samples_split = 2 )

model.fit(dataset_input[num_cols + encoded_cols] , dataset_target)

p = model.predict(dataset_input[num_cols + encoded_cols])

rmse = mean_squared_error(dataset_target , p , squared= False)

rmse

preds = model.predict(new_test[num_cols + encoded_cols])

len(preds)



"""Now lets train another model using XGBRegressor."""

from xgboost import XGBRegressor

?XGBRegressor

model1 = XGBRegressor(n_jobs = -1 ,
                     random_state = 42)

dataset_input[num_cols + encoded_cols]

model1.fit(dataset_input[num_cols + encoded_cols] , dataset_target)

preds1 = model1.predict(dataset_input[num_cols + encoded_cols])

rmse1 = mean_squared_error(dataset_target , preds1 , squared= False)

rmse1

imp_df1  = pd.DataFrame({'features' : dataset_input[num_cols + encoded_cols].columns , 'importance' : model.feature_importances_}).sort_values('importance', ascending = False)

imp_df1



def test_params1(**params):
  model = XGBRegressor(n_jobs = -1 ,
                     random_state = 42,
                     **params)
  model.fit(X_train , train_targets)
  train_rmse = mean_squared_error(train_targets , model.predict(X_train) , squared=False)
  val_rmse = mean_squared_error(val_targets , model.predict(X_val) , squared= False)
  return train_rmse , val_rmse

def test_params_and_plot1(param_name , param_values):
  train_errors, val_errors = [], [] 
  for value in param_values :
    params  = { param_name : value}
    train_rmse , val_rmse = test_params1(**params)
    train_errors.append(train_rmse)
    val_errors.append(val_rmse)
  plt.figure(figsize=(10,6))
  plt.title('Overfitting curve : ' + param_name)
  plt.plot(param_values , train_errors , 'b-o')
  plt.plot(param_values , val_errors , 'r-o')
  plt.xlabel(param_name)
  plt.ylabel('rmse')
  plt.legend(['Training', 'validation'])

test_params_and_plot1('learning_rate' ,[0.2 , 0.3 , 0.4 , 0.6 , 0.8 , 0.9])

test_params_and_plot1('n_estimators' , [20 , 40 , 60 , 80 , 100 , 150 , 200])

test_params_and_plot1('max_depth' , [ 5 , 10 , 15 , 20 ])

test_params_and_plot1('min_child_weight' , [1 , 2 , 4 , 8 , 16])

test_params_and_plot1('num_parallel_tree' , [1, 2 , 4 , 8 , 16 ])

test_params_and_plot1('booster' , ['gbtree', 'gblinear', 'dart'])

test_params_and_plot1('reg_alpha' , [0.2 , 0.4 , 0.6])

test_params_and_plot1('reg_lambda' , [0.1,0.2,0.4,0.8])

test_params_and_plot1('gamma' , [0.1 ,0.2 , 0.4 , 0.8])

test_params_and_plot1('colsample_bytree' , [0.1 , 0.2 ,  0.5 ,  0.7 , 0.9])

test_params_and_plot1('max_delta_step' , [0.1 , 0.2 ,  0.5 ,  0.7 , 0.9])

test_params_and_plot1('subsample' , [0.1 , 0.2 ,  0.5 ,  0.7 , 0.9])

test_params_and_plot1('colsample_bylevel' , [0.1 , 0.2 ,  0.5 ,  0.7 , 0.9])

test_params_and_plot1('colsample_bynode' , [0.1 , 0.2 ,  0.5 ,  0.7 , 0.9])

test_params_and_plot1('scale_pos_weight' , [0.1 , 0.2 ,  0.5 ,  0.7 , 0.9])

test_params_and_plot1('base_score' , [0.1 , 0.2 ,  0.5 ,  0.7 , 0.9])





model1 = XGBRegressor(base_score=0.5, booster='gbtree',
             learning_rate=0.9, max_delta_step=0, max_depth=10,
             min_child_weight=4, 
             n_estimators=1000, n_jobs=-1, random_state=42, reg_lambda=0.4, scale_pos_weight=1)

model1.fit(dataset_input[num_cols + encoded_cols] , dataset_target)

preds1 = model1.predict(dataset_input[num_cols + encoded_cols])

rmse1 =  mean_squared_error(preds1 , dataset_target , squared= False)

rmse1

predict1 = model1.predict(new_test[num_cols + encoded_cols])

predict1

"""Taking average of prediction by RandForestRegressor and XGBRegressor """

final_predict = (preds + predict1)/2

sampleSubmission_df

"""Updating the samplesubmission with our prediction for submisssion"""

sampleSubmission_df['Weekly_Sales'] = final_predict

sampleSubmission_df

jovian.commit()